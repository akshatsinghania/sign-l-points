{"cells":[{"cell_type":"markdown","metadata":{"id":"3HWPscG6qbGz"},"source":["# 1. Import and Install Dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5invWmlmqbG1","executionInfo":{"status":"ok","timestamp":1675384752528,"user_tz":-330,"elapsed":98732,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}},"outputId":"81eeccca-a20b-47a2-b4f1-1e8a53cfc0b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow==2.4.1\n","  Downloading tensorflow-2.4.1-cp38-cp38-manylinux2010_x86_64.whl (394.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.4/394.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-gpu==2.4.1\n","  Downloading tensorflow_gpu-2.4.1-cp38-cp38-manylinux2010_x86_64.whl (394.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.4/394.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (4.6.0.66)\n","Collecting mediapipe\n","  Downloading mediapipe-0.9.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sklearn\n","  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.1) (3.19.6)\n","Collecting wrapt~=1.12.1\n","  Downloading wrapt-1.12.1.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.1) (1.12)\n","Collecting h5py~=2.10.0\n","  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting termcolor~=1.1.0\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting absl-py~=0.10\n","  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions~=3.7.4\n","  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.1) (0.38.4)\n","Collecting tensorflow-estimator<2.5.0,>=2.4.0\n","  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.1) (1.6.3)\n","Collecting numpy~=1.19.2\n","  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.1) (3.3.0)\n","Collecting grpcio~=1.32.0\n","  Downloading grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gast==0.3.3\n","  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.1) (2.9.1)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.1) (0.2.0)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.1) (1.15.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.1) (1.1.2)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (22.2.0)\n","Collecting mediapipe\n","  Downloading mediapipe-0.9.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading mediapipe-0.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading mediapipe-0.8.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.8/dist-packages (from mediapipe) (4.6.0.66)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.4.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.6)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.25.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.16.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (57.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (5.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (6.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.24.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.12.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.2.2)\n","Building wheels for collected packages: sklearn, termcolor, wrapt\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=a453c8e5b8ebd3358ba371299dd7aa2fd9ad329ab4fdccecd85f67730d429406\n","  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4849 sha256=b90e24cef58b7f77dde7341cedc7dd6261a89d4bf8f83fea734468a4da27ef32\n","  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n","  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=78570 sha256=ca4ef48afcaaead9f8cac8a850a93a90d7fb8c8b00eddd53bd8e73281558f05f\n","  Stored in directory: /root/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n","Successfully built sklearn termcolor wrapt\n","Installing collected packages: wrapt, typing-extensions, termcolor, tensorflow-estimator, sklearn, numpy, grpcio, gast, absl-py, h5py, mediapipe, tensorflow-gpu, tensorflow\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.14.1\n","    Uninstalling wrapt-1.14.1:\n","      Successfully uninstalled wrapt-1.14.1\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.4.0\n","    Uninstalling typing_extensions-4.4.0:\n","      Successfully uninstalled typing_extensions-4.4.0\n","  Attempting uninstall: termcolor\n","    Found existing installation: termcolor 2.2.0\n","    Uninstalling termcolor-2.2.0:\n","      Successfully uninstalled termcolor-2.2.0\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.9.0\n","    Uninstalling tensorflow-estimator-2.9.0:\n","      Successfully uninstalled tensorflow-estimator-2.9.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.51.1\n","    Uninstalling grpcio-1.51.1:\n","      Successfully uninstalled grpcio-1.51.1\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Attempting uninstall: absl-py\n","    Found existing installation: absl-py 1.4.0\n","    Uninstalling absl-py-1.4.0:\n","      Successfully uninstalled absl-py-1.4.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.9.2\n","    Uninstalling tensorflow-2.9.2:\n","      Successfully uninstalled tensorflow-2.9.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","sqlalchemy 2.0.0 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","pydantic 1.10.4 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","jaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","jax 0.3.25 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.32.0 which is incompatible.\n","google-cloud-bigquery 3.4.2 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.32.0 which is incompatible.\n","cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed absl-py-0.15.0 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 mediapipe-0.8.11 numpy-1.19.5 sklearn-0.0.post1 tensorflow-2.4.1 tensorflow-estimator-2.4.0 tensorflow-gpu-2.4.1 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"]}],"source":["!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"wSTIAaCAqbG2","executionInfo":{"status":"ok","timestamp":1675384756292,"user_tz":-330,"elapsed":3777,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["import os\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","from tensorflow.keras.models import Sequential\n","from scipy import stats\n","import cv2\n","from tensorflow.keras.layers import LSTM, Dense\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n","import mediapipe as mp\n","import time"]},{"cell_type":"markdown","metadata":{"id":"SVToCXaxqbG2"},"source":["# 2. Keypoints using MP Holistic"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"joSvVT0_qbG3","executionInfo":{"status":"ok","timestamp":1675384756294,"user_tz":-330,"elapsed":12,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["mp_holistic = mp.solutions.holistic # Holistic model\n","mp_drawing = mp.solutions.drawing_utils # Drawing utilities"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"WpNk5ppoqbG3","executionInfo":{"status":"ok","timestamp":1675384756294,"user_tz":-330,"elapsed":10,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["def mediapipe_detection(image, model):\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n","    image.flags.writeable = False                  # Image is no longer writeable\n","    results = model.process(image)                 # Make prediction\n","    image.flags.writeable = True                   # Image is now writeable \n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n","    return image, results"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"tIewuNZXqbG3","executionInfo":{"status":"ok","timestamp":1675384756294,"user_tz":-330,"elapsed":9,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["def draw_landmarks(image, results):\n","    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n","    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n","    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n","    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_ss4oJ7kqbG4","executionInfo":{"status":"ok","timestamp":1675384756295,"user_tz":-330,"elapsed":9,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["def draw_styled_landmarks(image, results):\n","    # Draw face connections\n","    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS, \n","                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n","                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n","                             ) \n","    # Draw pose connections\n","    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n","                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n","                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n","                             ) \n","    # Draw left hand connections\n","    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n","                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n","                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n","                             ) \n","    # Draw right hand connections  \n","    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n","                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n","                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n","                             ) "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"j5T8zX_hqbG4","executionInfo":{"status":"ok","timestamp":1675384756766,"user_tz":-330,"elapsed":480,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["cap = cv2.VideoCapture(0)\n","# Set mediapipe model \n","with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","    while cap.isOpened():\n","\n","        # Read feed\n","        ret, frame = cap.read()\n","\n","        # Make detections\n","        image, results = mediapipe_detection(frame, holistic)\n","        print(results)\n","        \n","        # Draw landmarks\n","        draw_styled_landmarks(image, results)\n","\n","        # Show to screen\n","        cv2.imshow('OpenCV Feed', image)\n","\n","        # Break gracefully\n","        if cv2.waitKey(10) & 0xFF == ord('q'):\n","            break\n","    cap.release()\n","    cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"id":"lHu38vuGqbG5","executionInfo":{"status":"error","timestamp":1675384756767,"user_tz":-330,"elapsed":28,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}},"outputId":"c4233345-c185-49f4-a6e4-1429bf08ad87"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-1b5405974159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdraw_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'frame' is not defined"]}],"source":["draw_landmarks(frame, results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sOsMA8lqbG5","executionInfo":{"status":"aborted","timestamp":1675384756768,"user_tz":-330,"elapsed":27,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"]},{"cell_type":"markdown","metadata":{"id":"N00FRKaOqbG5"},"source":["# 3. Extract Keypoint Values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qkh4zEchqbG6","executionInfo":{"status":"aborted","timestamp":1675384756768,"user_tz":-330,"elapsed":26,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["len(results.left_hand_landmarks.landmark)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5QTTpGK1qbG6","executionInfo":{"status":"aborted","timestamp":1675384756769,"user_tz":-330,"elapsed":27,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["pose = []\n","for res in results.pose_landmarks.landmark:\n","    test = np.array([res.x, res.y, res.z, res.visibility])\n","    pose.append(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tI9uTn-cqbG6","executionInfo":{"status":"aborted","timestamp":1675384756769,"user_tz":-330,"elapsed":27,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n","face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n","lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n","rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FbifIq_qbG6","executionInfo":{"status":"aborted","timestamp":1675384756770,"user_tz":-330,"elapsed":28,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() \n","    if results.face_landmarks \n","    else np.zeros(1404)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"13YF_593qbG6","executionInfo":{"status":"aborted","timestamp":1675384756770,"user_tz":-330,"elapsed":27,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["def extract_keypoints(results):\n","    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n","    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n","    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n","    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n","    return np.concatenate([pose, face, lh, rh])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lav7v2_SqbG6","executionInfo":{"status":"aborted","timestamp":1675384756770,"user_tz":-330,"elapsed":27,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["result_test = extract_keypoints(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O0cKvYSsqbG6","executionInfo":{"status":"aborted","timestamp":1675384756771,"user_tz":-330,"elapsed":28,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["result_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2rXrWhtDqbG7","executionInfo":{"status":"aborted","timestamp":1675384756771,"user_tz":-330,"elapsed":28,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["np.save('0', result_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tP4n_TY3qbG7","executionInfo":{"status":"aborted","timestamp":1675384756772,"user_tz":-330,"elapsed":29,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["np.load('0.npy')"]},{"cell_type":"markdown","metadata":{"id":"B9Xkk8B8qbG7"},"source":["# 4. Setup Folders for Collection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znKJQga7qbG7","executionInfo":{"status":"aborted","timestamp":1675384756772,"user_tz":-330,"elapsed":28,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["# Path for exported data, numpy arrays\n","DATA_PATH = os.path.join('MP_Data') \n","\n","# Actions that we try to detect\n","actions = np.array(['hello', 'thanks', 'iloveyou'])\n","\n","# Thirty videos worth of data\n","no_sequences = 30\n","\n","# Videos are going to be 30 frames in length\n","sequence_length = 30\n","\n","# Folder start\n","start_folder = 30"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jL2Z_K8zqbG7","executionInfo":{"status":"aborted","timestamp":1675384756772,"user_tz":-330,"elapsed":28,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["for action in actions: \n","    dirmax = np.max(np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int))\n","    for sequence in range(1,no_sequences+1):\n","        try: \n","            os.makedirs(os.path.join(DATA_PATH, action, str(dirmax+sequence)))\n","        except:\n","            pass"]},{"cell_type":"markdown","metadata":{"id":"nRV-SmCZqbG7"},"source":["# 5. Collect Keypoint Values for Training and Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQUpZVbcqbG7","executionInfo":{"status":"aborted","timestamp":1675384756772,"user_tz":-330,"elapsed":28,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["cap = cv2.VideoCapture(0)\n","# Set mediapipe model \n","with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","    \n","    # NEW LOOP\n","    # Loop through actions\n","    for action in actions:\n","        # Loop through sequences aka videos\n","        for sequence in range(start_folder, start_folder+no_sequences):\n","            # Loop through video length aka sequence length\n","            for frame_num in range(sequence_length):\n","\n","                # Read feed\n","                ret, frame = cap.read()\n","\n","                # Make detections\n","                image, results = mediapipe_detection(frame, holistic)\n","\n","                # Draw landmarks\n","                draw_styled_landmarks(image, results)\n","                \n","                # NEW Apply wait logic\n","                if frame_num == 0: \n","                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n","                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n","                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n","                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n","                    # Show to screen\n","                    cv2.imshow('OpenCV Feed', image)\n","                    cv2.waitKey(500)\n","                else: \n","                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n","                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n","                    # Show to screen\n","                    cv2.imshow('OpenCV Feed', image)\n","                \n","                # NEW Export keypoints\n","                keypoints = extract_keypoints(results)\n","                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n","                np.save(npy_path, keypoints)\n","\n","                # Break gracefully\n","                if cv2.waitKey(10) & 0xFF == ord('q'):\n","                    break\n","                    \n","    cap.release()\n","    cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8H5GAFyTqbG7","executionInfo":{"status":"aborted","timestamp":1675384756773,"user_tz":-330,"elapsed":29,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"xCIbCoNsqbG7"},"source":["# 6. Preprocess Data and Create Labels and Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCU3-FdnqbG7","executionInfo":{"status":"aborted","timestamp":1675384756773,"user_tz":-330,"elapsed":28,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NInWG3kDqbG8","executionInfo":{"status":"aborted","timestamp":1675384756773,"user_tz":-330,"elapsed":28,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["label_map = {label:num for num, label in enumerate(actions)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yusvWv8SqbG8","executionInfo":{"status":"aborted","timestamp":1675384756773,"user_tz":-330,"elapsed":28,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["label_map"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7P43fPwVqbG8","executionInfo":{"status":"aborted","timestamp":1675384756773,"user_tz":-330,"elapsed":28,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["sequences, labels = [], []\n","for action in actions:\n","    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n","        window = []\n","        for frame_num in range(sequence_length):\n","            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n","            window.append(res)\n","        sequences.append(window)\n","        labels.append(label_map[action])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kq7rGIW7qbG8","executionInfo":{"status":"aborted","timestamp":1675384756776,"user_tz":-330,"elapsed":30,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["np.array(sequences).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLoVw3iEqbG8","executionInfo":{"status":"aborted","timestamp":1675384756776,"user_tz":-330,"elapsed":30,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["np.array(labels).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8MuaSmmqbG8","executionInfo":{"status":"aborted","timestamp":1675384756777,"user_tz":-330,"elapsed":31,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["X = np.array(sequences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3O92pHeNqbG8","executionInfo":{"status":"aborted","timestamp":1675384756778,"user_tz":-330,"elapsed":32,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIL6B6vhqbG8","executionInfo":{"status":"aborted","timestamp":1675384756778,"user_tz":-330,"elapsed":32,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["y = to_categorical(labels).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XBOSrh0qqbG8","executionInfo":{"status":"aborted","timestamp":1675384756778,"user_tz":-330,"elapsed":32,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5k43Jh12qbG8","executionInfo":{"status":"aborted","timestamp":1675384756779,"user_tz":-330,"elapsed":33,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["y_test.shape"]},{"cell_type":"markdown","metadata":{"id":"TTS5qUWCqbG9"},"source":["# 7. Build and Train LSTM Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IxmvNWufqbG9","executionInfo":{"status":"aborted","timestamp":1675384756780,"user_tz":-330,"elapsed":34,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from tensorflow.keras.callbacks import TensorBoard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QqVjqQXnqbG9","executionInfo":{"status":"aborted","timestamp":1675384756780,"user_tz":-330,"elapsed":34,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["log_dir = os.path.join('Logs')\n","tb_callback = TensorBoard(log_dir=log_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzotQD5tqbG9","executionInfo":{"status":"aborted","timestamp":1675384756780,"user_tz":-330,"elapsed":34,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["model = Sequential()\n","model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n","model.add(LSTM(128, return_sequences=True, activation='relu'))\n","model.add(LSTM(64, return_sequences=False, activation='relu'))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(actions.shape[0], activation='softmax'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iv265FIqbG9","executionInfo":{"status":"aborted","timestamp":1675384756780,"user_tz":-330,"elapsed":34,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"RtU_u7VUqbG9","executionInfo":{"status":"aborted","timestamp":1675384756780,"user_tz":-330,"elapsed":34,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzlXI4kCqbG9","executionInfo":{"status":"aborted","timestamp":1675384756781,"user_tz":-330,"elapsed":35,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"8wTdj57_qbG9"},"source":["# 8. Make Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oK_8c4IlqbG9","executionInfo":{"status":"aborted","timestamp":1675384756781,"user_tz":-330,"elapsed":35,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["res = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1I6en6B4qbG9","executionInfo":{"status":"aborted","timestamp":1675384756781,"user_tz":-330,"elapsed":34,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["actions[np.argmax(res[4])]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xj0ExjM3qbG9","executionInfo":{"status":"aborted","timestamp":1675384756781,"user_tz":-330,"elapsed":34,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["actions[np.argmax(y_test[4])]"]},{"cell_type":"markdown","metadata":{"id":"r-OD1N8wqbG9"},"source":["# 9. Save Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WvelDkSYqbG9","executionInfo":{"status":"aborted","timestamp":1675384756781,"user_tz":-330,"elapsed":34,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["model.save('action.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10rUlAmJqbG-","executionInfo":{"status":"aborted","timestamp":1675384756782,"user_tz":-330,"elapsed":35,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["del model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6IpVROkqbG-","executionInfo":{"status":"aborted","timestamp":1675384756782,"user_tz":-330,"elapsed":35,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["model.load_weights('action.h5')"]},{"cell_type":"markdown","metadata":{"id":"iDX3xMSbqbG-"},"source":["# 10. Evaluation using Confusion Matrix and Accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tr1jvceBqbG-","executionInfo":{"status":"aborted","timestamp":1675384756782,"user_tz":-330,"elapsed":35,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mjr1VdLhqbG-","executionInfo":{"status":"aborted","timestamp":1675384756782,"user_tz":-330,"elapsed":35,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["yhat = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1uBR7NaqbG-","executionInfo":{"status":"aborted","timestamp":1675384756783,"user_tz":-330,"elapsed":36,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["ytrue = np.argmax(y_test, axis=1).tolist()\n","yhat = np.argmax(yhat, axis=1).tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOUkIIQ3qbG-","executionInfo":{"status":"aborted","timestamp":1675384756783,"user_tz":-330,"elapsed":36,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["multilabel_confusion_matrix(ytrue, yhat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YW_a7_nkqbG-","executionInfo":{"status":"aborted","timestamp":1675384756784,"user_tz":-330,"elapsed":37,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["accuracy_score(ytrue, yhat)"]},{"cell_type":"markdown","metadata":{"id":"5B-Nfox3qbG-"},"source":["# 11. Test in Real Time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7YJ-XdpYqbG-","executionInfo":{"status":"aborted","timestamp":1675384756784,"user_tz":-330,"elapsed":37,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["from scipy import stats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sg4MUrNoqbG-","executionInfo":{"status":"aborted","timestamp":1675384756785,"user_tz":-330,"elapsed":38,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["colors = [(245,117,16), (117,245,16), (16,117,245)]\n","def prob_viz(res, actions, input_frame, colors):\n","    output_frame = input_frame.copy()\n","    for num, prob in enumerate(res):\n","        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n","        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n","        \n","    return output_frame"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"mqnodU2nqbG-","executionInfo":{"status":"aborted","timestamp":1675384756785,"user_tz":-330,"elapsed":38,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["plt.figure(figsize=(18,18))\n","plt.imshow(prob_viz(res, actions, image, colors))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZU4Em0_cqbG-","executionInfo":{"status":"aborted","timestamp":1675384756785,"user_tz":-330,"elapsed":38,"user":{"displayName":"Akshat Singhania","userId":"11590367798485926491"}}},"outputs":[],"source":["# 1. New detection variables\n","sequence = []\n","sentence = []\n","predictions = []\n","threshold = 0.5\n","\n","cap = cv2.VideoCapture(0)\n","# Set mediapipe model \n","with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","    while cap.isOpened():\n","\n","        # Read feed\n","        ret, frame = cap.read()\n","\n","        # Make detections\n","        image, results = mediapipe_detection(frame, holistic)\n","        print(results)\n","        \n","        # Draw landmarks\n","        draw_styled_landmarks(image, results)\n","        \n","        # 2. Prediction logic\n","        keypoints = extract_keypoints(results)\n","        sequence.append(keypoints)\n","        sequence = sequence[-30:]\n","        \n","        if len(sequence) == 30:\n","            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n","            print(actions[np.argmax(res)])\n","            predictions.append(np.argmax(res))\n","            \n","            \n","        #3. Viz logic\n","            if np.unique(predictions[-10:])[0]==np.argmax(res): \n","                if res[np.argmax(res)] > threshold: \n","                    \n","                    if len(sentence) > 0: \n","                        if actions[np.argmax(res)] != sentence[-1]:\n","                            sentence.append(actions[np.argmax(res)])\n","                    else:\n","                        sentence.append(actions[np.argmax(res)])\n","\n","            if len(sentence) > 5: \n","                sentence = sentence[-5:]\n","\n","            # Viz probabilities\n","            image = prob_viz(res, actions, image, colors)\n","            \n","        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n","        cv2.putText(image, ' '.join(sentence), (3,30), \n","                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n","        \n","        # Show to screen\n","        cv2.imshow('OpenCV Feed', image)\n","\n","        # Break gracefully\n","        if cv2.waitKey(10) & 0xFF == ord('q'):\n","            break\n","    cap.release()\n","    cv2.destroyAllWindows()"]}],"metadata":{"kernelspec":{"display_name":"action","language":"python","name":"action"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}